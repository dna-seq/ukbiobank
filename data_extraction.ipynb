{"metadata": {"language_info": {"name": "python", "version": "3.9.16", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3 (ipykernel)", "language": "python"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "# Import packages\nimport pyspark\nimport dxpy\nimport dxdata\nfrom pyspark.sql import SparkSession", "metadata": {"trusted": true, "tags": []}, "execution_count": 2, "outputs": []}, {"cell_type": "code", "source": "dxdata.__version__", "metadata": {"trusted": true, "tags": []}, "execution_count": 3, "outputs": [{"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "'0.41.0'"}, "metadata": {}}]}, {"cell_type": "code", "source": "# Spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)\n\n\n# spark = SparkSession.builder \\\n#     .appName(\"MyApp\") \\\n#     .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n#     .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n#     .getOrCreate()\n\n# spark = SparkSession.builder \\\n#     .appName(\"MyApp\") \\\n#     .config(\"spark.driver.maxResultSize\", \"4g\") \\\n#     .config(\"spark.kryoserializer.buffer.max\", \"2048m\") \\\n#     .getOrCreate()\n\nconf = pyspark.SparkConf().setAll([('spark.driver.maxResultSize', '2047'), (\"spark.kryoserializer.buffer.max\", \"1g\")])", "metadata": {"trusted": true, "tags": []}, "execution_count": 4, "outputs": []}, {"cell_type": "code", "source": "# conf = spark.sparkContext.getConf()\n\n# Print the values of the specific configurations\nprint(\"spark.kryoserializer.buffer.max:\", conf.get(\"spark.kryoserializer.buffer.max\"))\nprint(\"spark.driver.maxResultSize:\", conf.get(\"spark.driver.maxResultSize\"))", "metadata": {"trusted": true, "tags": []}, "execution_count": 5, "outputs": [{"name": "stdout", "text": "spark.kryoserializer.buffer.max: 1g\nspark.driver.maxResultSize: 2047\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# List all Spark configurations to verify settings\nfor key, value in conf.getAll():\n    print(f\"{key}: {value}\")\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 6, "outputs": [{"name": "stdout", "text": "spark.port.maxRetries: 16\nspark.eventLog.enabled: true\nspark.driver.defaultJavaOptions: -Ddnanexus.fs.output.committer.pendingdirname=job-GkkFpgjJfg3xyj9XpVy3ZF7Z\nspark.blockManager.port: 44000\nspark.driver.port: 42000\nspark.driver.host: ip-10-60-42-98.eu-west-2.compute.internal\nspark.executor.memory: 2800m\nspark.kryo.registrator: is.hail.kryo.HailKryoRegistrator\nspark.serializer: org.apache.spark.serializer.KryoSerializer\nspark.sql.shuffle.partitions: 5\nspark.history.fs.logDirectory: hdfs://master:9000/eventlogs/\nspark.repl.local.jars: local:/cluster/dnax/jars/dnax-common-1.0.jar,local:/cluster/dnax/jars/dnaxfilesystem-1.0.jar,local:/cluster/dnax/jars/hiveclient-1.0.jar,local:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar\nspark.executor.extraClassPath: /cluster/dnax/jars/dnax-common-1.0.jar:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar:/cluster/dnax/jars/dnaxspark-1.0.jar:/cluster/spark/jars/hail-all-spark-0.2.116.jar\nspark.executor.defaultJavaOptions: -Ddnanexus.fs.output.committer.pendingdirname=job-GkkFpgjJfg3xyj9XpVy3ZF7Z\nspark.submit.deployMode: client\nspark.sql.adaptive.enabled: false\nspark.executorEnv.PYTHONPATH: /opt/jupyterlab_dx_extension::/cluster/spark/python:/cluster/spark/python/lib/py4j-0.10.9.5-src.zip\nspark.driver.maxResultSize: 2047\nspark.driver.memory: 2800m\nspark.driver.blockManager.port: 43000\nspark.eventLog.dir: hdfs://master:9000/eventlogs/\nspark.sql.parquet.int96RebaseModeInWrite: LEGACY\nspark.driver.extraClassPath: /cluster/dnax/jars/dnax-common-1.0.jar:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar:/cluster/dnax/jars/dnaxspark-1.0.jar:/cluster/spark/jars/hail-all-spark-0.2.116.jar\nspark.ui.enabled: true\nspark.sql.warehouse.dir: /cluster/spark/spark-warehouse\nspark.sql.parquet.int96RebaseModeInRead: LEGACY\nspark.executor.cores: 2\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: 2\nspark.app.name: pyspark-shell\nspark.ui.port: 8081\nspark.hadoop.fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\nspark.sql.catalogImplementation: hive\nspark.driver.bindAddress: 0.0.0.0\nspark.kryoserializer.buffer.max: 1g\nspark.submit.pyFiles: \nspark.jars: local:/cluster/dnax/jars/dnax-common-1.0.jar,local:/cluster/dnax/jars/dnaxfilesystem-1.0.jar,local:/cluster/dnax/jars/hiveclient-1.0.jar,local:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar\nspark.master: spark://master:41000\nspark.ui.showConsoleProgress: true\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# Automatically discover dispensed database name and dataset id\ndispensed_database = dxpy.find_one_data_object(\n    classname='database', \n    name='app*', \n    folder='/', \n    name_mode='glob', \n    describe=True)\ndispensed_database_name = dispensed_database['describe']['name']\n\ndispensed_dataset = dxpy.find_one_data_object(\n    typename='Dataset', \n    name='app*.dataset', \n    folder='/', \n    name_mode='glob')\ndispensed_dataset_id = dispensed_dataset['id']\nprint('dispensed_dataset_id:', dispensed_dataset_id)", "metadata": {"trusted": true, "tags": []}, "execution_count": 2, "outputs": [{"name": "stdout", "text": "dispensed_dataset_id: record-GZ85K5jJYvKZJ6kG1yxfzbzJ\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "# Access dataset", "metadata": {"tags": []}}, {"cell_type": "code", "source": "dataset = dxdata.load_dataset(id=dispensed_dataset_id)", "metadata": {"trusted": true, "tags": []}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": "dataset.entities", "metadata": {"trusted": true, "tags": []}, "execution_count": 7, "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": "[<Entity \"participant\">,\n <Entity \"death\">,\n <Entity \"hesin_oper\">,\n <Entity \"death_cause\">,\n <Entity \"hesin_diag\">,\n <Entity \"hesin_psych\">,\n <Entity \"hesin_maternity\">,\n <Entity \"hesin\">,\n <Entity \"hesin_delivery\">,\n <Entity \"hesin_critical\">,\n <Entity \"covid19_result_england\">,\n <Entity \"covid19_result_scotland\">,\n <Entity \"covid19_result_wales\">,\n <Entity \"gp_clinical\">,\n <Entity \"gp_registrations\">,\n <Entity \"gp_scripts\">,\n <Entity \"omop_visit_occurrence\">,\n <Entity \"omop_dose_era\">,\n <Entity \"omop_drug_era\">,\n <Entity \"omop_drug_exposure\">,\n <Entity \"omop_note\">,\n <Entity \"omop_observation\">,\n <Entity \"omop_death\">,\n <Entity \"omop_device_exposure\">,\n <Entity \"omop_condition_era\">,\n <Entity \"omop_condition_occurrence\">,\n <Entity \"omop_measurement\">,\n <Entity \"omop_procedure_occurrence\">,\n <Entity \"omop_specimen\">,\n <Entity \"omop_observation_period\">,\n <Entity \"omop_person\">,\n <Entity \"omop_visit_detail\">,\n <Entity \"olink_instance_0\">,\n <Entity \"olink_instance_2\">,\n <Entity \"olink_instance_3\">]"}, "metadata": {}}]}, {"cell_type": "code", "source": "participant = dataset['participant']", "metadata": {"trusted": true, "tags": []}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": "# Returns all field objects for a given UKB showcase field id\n\ndef fields_for_id(field_id):\n    from distutils.version import LooseVersion\n    field_id = str(field_id)\n    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n    return sorted(fields, key=lambda f: LooseVersion(f.name))\n\n# Returns all field names for a given UKB showcase field id\n\ndef field_names_for_id(field_id):\n    return [f.name for f in fields_for_id(field_id)]", "metadata": {"trusted": true, "tags": []}, "execution_count": 4, "outputs": []}, {"cell_type": "markdown", "source": "# Extraction examples", "metadata": {}}, {"cell_type": "code", "source": "# Participant sex\nprint(field_names_for_id('31'))", "metadata": {"trusted": true, "tags": []}, "execution_count": 13, "outputs": [{"name": "stdout", "text": "['p31']\n", "output_type": "stream"}, {"name": "stderr", "text": "/tmp/ipykernel_84/339987039.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return sorted(fields, key=lambda f: LooseVersion(f.name))\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# Age when attending assessment centre has multiple instances (visits) \nfield_names_for_id('21003')", "metadata": {"trusted": true, "tags": []}, "execution_count": 16, "outputs": [{"name": "stderr", "text": "/tmp/ipykernel_84/339987039.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return sorted(fields, key=lambda f: LooseVersion(f.name))\n", "output_type": "stream"}, {"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "['p21003_i0', 'p21003_i1', 'p21003_i2', 'p21003_i3']"}, "metadata": {}}]}, {"cell_type": "code", "source": "# Pulse rate has multiple instances and array indices (measured twice in each visit)\nfield_names_for_id('102')", "metadata": {"trusted": true, "tags": []}, "execution_count": 17, "outputs": [{"name": "stderr", "text": "/tmp/ipykernel_84/339987039.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return sorted(fields, key=lambda f: LooseVersion(f.name))\n", "output_type": "stream"}, {"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "['p102_i0_a0',\n 'p102_i0_a1',\n 'p102_i1_a0',\n 'p102_i1_a1',\n 'p102_i2_a0',\n 'p102_i2_a1',\n 'p102_i3_a0',\n 'p102_i3_a1']"}, "metadata": {}}]}, {"cell_type": "code", "source": "field_ids = ['31','21003', '102']\n# sum flattens list of lists\nfield_ids = sum([field_names_for_id(field_id) for field_id in field_ids], []) \nfield_ids", "metadata": {"trusted": true, "tags": []}, "execution_count": 10, "outputs": [{"name": "stderr", "text": "/tmp/ipykernel_88/339987039.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return sorted(fields, key=lambda f: LooseVersion(f.name))\n", "output_type": "stream"}, {"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": "['p31',\n 'p21003_i0',\n 'p21003_i1',\n 'p21003_i2',\n 'p21003_i3',\n 'p102_i0_a0',\n 'p102_i0_a1',\n 'p102_i1_a0',\n 'p102_i1_a1',\n 'p102_i2_a0',\n 'p102_i2_a1',\n 'p102_i3_a0',\n 'p102_i3_a1']"}, "metadata": {}}]}, {"cell_type": "code", "source": "field_ids = ['p31']\n\n#subset_participants = participant_df.head(10)\n#subset_participants", "metadata": {"trusted": true, "tags": []}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": "df = participant.retrieve_fields(names=field_ids, coding_values='replace', engine=dxdata.connect())", "metadata": {"trusted": true, "tags": []}, "execution_count": 13, "outputs": []}, {"cell_type": "code", "source": "# df.show(5, truncate=False)", "metadata": {"trusted": true, "tags": []}, "execution_count": 14, "outputs": []}, {"cell_type": "code", "source": "df.toPandas().to_csv('test_extraction.tsv', sep='\\t', index=False)", "metadata": {"trusted": true, "tags": []}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "%%bash\ndx upload test_extraction.tsv --dest /Test_Project_Start/", "metadata": {"trusted": true, "tags": []}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "# Blood and disease data extraction", "metadata": {}}, {"cell_type": "code", "source": "common_info = ['eid', 'p31', 'p21022', 'p34', 'p52', 'p23165'] # sex, Age at recruitment, year, month of birth, 23165: Blood-type haplotype\nconditions = ['21003', '2188', '2306', '20022', '20001', '20002', '3079'] # 21003: Age when attended assessment centre, 2188: Long-standing illness, disability or infirmity, 2306: Weight change compared with 1 year ago, 20022: Birth weight, 20001: cancer-code, self-reported; 20002: Non-cancer illness code, self-reported (https://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20002); 3079: pace-maker\nblood_count =  ['30160', '30220', '30150', '30210', '30030', '30020', '30300', '30290', '30280', '30120', '30180', '30050', '30060', '30040', '30100', '30260', '30270', '30130', '30190', '30140', '30200', '30170', '30230', '30080', '30090', '30110', '30010', '30070', '30250', '30240', '30000']\nblood_biochemistry = ['30897', '30622', '30621', '30623', '30624', '30625', '30626', '30602', '30601', '30603', '30604', '30605', '30606', '30612', '30611', '30613', '30614', '30615', '30616', '30632', '30631', '30633', '30634', '30635', '30636', '30642', '30641', '30643', '30644', '30645', '30646', '30652', '30651', '30653', '30654', '30655', '30656', '30712', '30711', '30713', '30714', '30715', '30716', '30682', '30681', '30683', '30684', '30685', '30686', '30692', '30691', '30693', '30694', '30695', '30696', '30702', '30701', '30703', '30704', '30705', '30706', '30722', '30721', '30723', '30724', '30725', '30726', '30662', '30661', '30663', '30664', '30665', '30666', '30732', '30731', '30733', '30734', '30735', '30736', '30742', '30741', '30743', '30744', '30745', '30746', '30751', '30753', '30754', '30755', '30756', '30762', '30761', '30763', '30764', '30765', '30766', '30772', '30771', '30773', '30774', '30775', '30776', '30782', '30781', '30783', '30784', '30785', '30786', '30792', '30791', '30793', '30794', '30795', '30796', '30802', '30801', '30803', '30804', '30805', '30806', '30812', '30811', '30813', '30814', '30815', '30816', '30822', '30821', '30823', '30824', '30825', '30826', '30832', '30831', '30833', '30834', '30835', '30836', '30852', '30851', '30853', '30854', '30855', '30856', '30842', '30841', '30843', '30844', '30845', '30846', '30862', '30861', '30863', '30864', '30865', '30866', '30872', '30871', '30873', '30874', '30875', '30876', '30882', '30881', '30883', '30884', '30885', '30886', '30672', '30671', '30673', '30674', '30675', '30676', '30892', '30891', '30893', '30894', '30895', '30896']\nmetabolomics =  ['23474', '23475', '23476', '23477', '23460', '23479', '23440', '23439', '23441', '23433', '23432', '23431', '23484', '23526', '23561', '23533', '23498', '23568', '23540', '23505', '23575', '23547', '23512', '23554', '23491', '23519', '23580', '23610', '23635', '23615', '23590', '23640', '23620', '23595', '23645', '23625', '23600', '23630', '23585', '23605', '23485', '23418', '23527', '23417', '23562', '23534', '23499', '23569', '23541', '23506', '23576', '23548', '23513', '23416', '23555', '23492', '23520', '23581', '23611', '23636', '23616', '23591', '23641', '23621', '23596', '23646', '23626', '23601', '23631', '23586', '23606', '23473', '23404', '23481', '23430', '23523', '23429', '23558', '23530', '23495', '23565', '23537', '23502', '23572', '23544', '23509', '23428', '23551', '23488', '23516', '23478', '23443', '23450', '23457', '23486', '23422', '23528', '23421', '23563', '23535', '23500', '23570', '23542', '23507', '23577', '23549', '23514', '23420', '23556', '23493', '23521', '23582', '23612', '23637', '23617', '23592', '23642', '23622', '23597', '23647', '23627', '23602', '23632', '23587', '23607', '23470', '20280', '23461', '23462', '23480', '23406', '23463', '23465', '23405', '23471', '23466', '23449', '23456', '23447', '23454', '23444', '23451', '23445', '23459', '23452', '23468', '23437', '23434', '23483', '23414', '23525', '23413', '23560', '23532', '23497', '23567', '23539', '23504', '23574', '23546', '23511', '23412', '23553', '23490', '23518', '23579', '23609', '23634', '23614', '23589', '23639', '23619', '23594', '23644', '23624', '23599', '23629', '23584', '23604', '23446', '23458', '23453', '23472', '23402', '23448', '23455', '20281', '23438', '23400', '23401', '23436', '23464', '23427', '23415', '23442', '23419', '23482', '23426', '23524', '23425', '23559', '23531', '23496', '23423', '23566', '23538', '23503', '23573', '23545', '23510', '23424', '23552', '23489', '23517', '23411', '23407', '23487', '23410', '23529', '23409', '23564', '23536', '23501', '23571', '23543', '23508', '23578', '23550', '23515', '23408', '23557', '23494', '23522', '23435', '23583', '23613', '23638', '23618', '23593', '23643', '23623', '23598', '23648', '23628', '23603', '23633', '23588', '23608', '23469', '23403', '23467']\ninfectious = ['23000', '23001', '23049', '23048', '23026', '23039', '23043', '23018', '23030', '23031', '23006', '23004', '23042', '23016', '23017', '23025', '23024', '23023', '23022', '23010', '23011', '23027', '23015', '23029', '23032', '23014', '23028', '23019', '23041', '23037', '23013', '23044', '23003', '23040', '23005', '23002', '23034', '23033', '23012', '23020', '23038', '23009', '23008', '23007', '23021', '23035', '23036']", "metadata": {"trusted": true, "tags": []}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": "\nfields = common_info + sum([field_names_for_id(field_id) for field_id in conditions],[]) + \\\nsum([field_names_for_id(field_id) for field_id in blood_count],[]) + \\\nsum([field_names_for_id(field_id) for field_id in blood_biochemistry],[]) + \\\nsum([field_names_for_id(field_id) for field_id in metabolomics],[]) + \\\nsum([field_names_for_id(field_id) for field_id in infectious],[])\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 8, "outputs": [{"name": "stderr", "text": "/tmp/ipykernel_68/339987039.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return sorted(fields, key=lambda f: LooseVersion(f.name))\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "len(fields)", "metadata": {"trusted": true, "tags": []}, "execution_count": 9, "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "1078"}, "metadata": {}}]}, {"cell_type": "code", "source": "with open('field_names.txt', 'w') as file:\n    for item in fields:\n        file.write(\"%s\\n\" % item)", "metadata": {"trusted": true, "tags": []}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": "%%bash\ndx upload field_names.txt --path /", "metadata": {"trusted": true, "tags": []}, "execution_count": 13, "outputs": [{"name": "stdout", "text": "ID                          file-GkZv22QJfg3pGVz95v7VG0G7\nClass                       file\nProject                     project-GZ82qXQJfg3z7xv124xqBJpj\nFolder                      /\nName                        field_names.txt\nState                       closing\nVisibility                  visible\nTypes                       -\nProperties                  -\nTags                        -\nOutgoing links              -\nCreated                     Wed Jun 12 15:38:50 2024\nCreated by                  alina_grf\n via the job                job-GkZqbkjJfg3Xg10VYVJ91614\nLast modified               Wed Jun 12 15:38:51 2024\nMedia type                  \narchivalState               \"live\"\ncloudAccount                \"cloudaccount-dnanexus\"\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## Grabbing fields into a Spark DataFrame", "metadata": {"tags": []}}, {"cell_type": "code", "source": "df = participant.retrieve_fields(names=fields, coding_values='replace', engine=dxdata.connect())", "metadata": {"trusted": true, "tags": []}, "execution_count": 23, "outputs": []}, {"cell_type": "code", "source": "%%bash\ndx run table-exporter df -output 'blood_disease_extraction.tsv' -output_format \"TSV\" -coding_option \"REPLACE\"", "metadata": {"trusted": true, "tags": []}, "execution_count": 62, "outputs": [{"name": "stderr", "text": "usage: dx [-h] [--version] command ...\n\nDNAnexus Command-Line Client, API v1.0.0, client v0.351.0\n\ndx is a command-line client for interacting with the DNAnexus platform.  You\ncan log in, navigate, upload, organize and share your data, launch analyses,\nand more.  For a quick tour of what the tool can do, see\n\n  https://documentation.dnanexus.com/getting-started/tutorials/cli-quickstart#quickstart-for-cli\n\nFor a breakdown of dx commands by category, run \"dx help\".\n\ndx exits with exit code 3 if invalid input is provided or an invalid operation\nis requested, and exit code 1 if an internal error is encountered.  The latter\nusually indicate bugs in dx; please report them at\n\n  https://github.com/dnanexus/dx-toolkit/issues\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --env-help  Display help message for overriding environment\n              variables\n  --version   show program's version number and exit\n\ndx: error: unrecognized arguments: df -output blood_disease_extraction.tsv -output_format TSV -coding_option REPLACE\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)", "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdx run table-exporter df -output \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mblood_disease_extraction.tsv\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m -output_format \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTSV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m -coding_option \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mREPLACE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n", "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n", "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'dx run table-exporter df -output \\'blood_disease_extraction.tsv\\' -output_format \"TSV\" -coding_option \"REPLACE\"\\n'' returned non-zero exit status 2."], "ename": "CalledProcessError", "evalue": "Command 'b'dx run table-exporter df -output \\'blood_disease_extraction.tsv\\' -output_format \"TSV\" -coding_option \"REPLACE\"\\n'' returned non-zero exit status 2.", "output_type": "error"}]}, {"cell_type": "code", "source": "df_repartitioned = df.repartition(5000).persist()", "metadata": {"trusted": true, "tags": []}, "execution_count": 63, "outputs": []}, {"cell_type": "code", "source": "df_repartitioned.toPandas().to_csv('blood_disease_extraction.tsv', sep='\\t', index=False)", "metadata": {"trusted": true, "tags": []}, "execution_count": 64, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_repartitioned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblood_disease_extraction.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n", "File \u001b[0;32m/cluster/spark/python/pyspark/sql/pandas/conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    158\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    160\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n", "File \u001b[0;32m/cluster/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n", "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/cluster/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n", "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o16768.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 3767 tasks (1024.3 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2548)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2493)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2492)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2492)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1250)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1250)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1250)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2736)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2678)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2667)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:410)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"], "ename": "Py4JJavaError", "evalue": "An error occurred while calling o16768.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 3767 tasks (1024.3 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2548)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2493)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2492)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2492)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1250)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1250)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1250)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2736)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2678)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2667)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:410)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n", "output_type": "error"}]}, {"cell_type": "code", "source": "%%bash\ndx upload blood_disease_extraction.tsv --path /", "metadata": {}, "execution_count": null, "outputs": []}]}